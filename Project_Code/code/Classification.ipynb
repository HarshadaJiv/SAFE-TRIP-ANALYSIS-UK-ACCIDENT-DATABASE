{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the preprocessed data for Casualities\n",
    "\n",
    "df = pd.read_csv(\"../data/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns = 'Casualty_IMD_Decile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df #checking what this dataframe df looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #Studying about the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #Studying about the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Casualty_Severity'] \n",
    "#Casualty_Severity is the target column. We don't want to include this in the training data\n",
    "\n",
    "X = X.loc[:, X.columns != 'Accident_Index']\n",
    "#Accident_Index is key of the accident dataset. This is not a valid feature in training/testing data.\n",
    "\n",
    "X = X.loc[:, X.columns != 'Casualty_Reference']\n",
    "#Casualty_Reference is key of the casuality dataset. This is not a valid feature in training/testing data.\n",
    "\n",
    "\n",
    "y = df['Casualty_Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X #X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y #Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the dataset X and y into X_train, X_tests, and y_train, y_tests correspondingly.\n",
    "# Split ratio of 80:20% is used and random_state is used to produce the same datapoints every time (reproducible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: logistic regression - accuracy 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "\n",
    "# #Defining the LogisticRegression model. fit_intercept is the bias added the decision function.\n",
    "# model = GridSearchCV(LogisticRegression(),grid,cv=10)\n",
    "\n",
    "# # Fitting the training (X and y) data into the model. The model learns the data now.\n",
    "# model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1, fit_intercept=True, intercept_scaling=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#When C = 0.1, accuracy 82.8%\n",
    "#When C = 0.01, accuracy 82.8%\n",
    "#When C = 1, accuracy 82.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model learnt the data, we will try to predict y_pred for X_test\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the predicted data looks like\\\n",
    "\n",
    "# print(\"tuned hpyerparameters :(best parameters) \",model.best_params_)\n",
    "# print(\"accuracy :\",model.best_score_)\n",
    "\n",
    "print('Minimum of the predictions is ' + str(np.amin(y_pred)))\n",
    "print('Maximum of the predictions is ' + str(np.amax(y_pred)))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got 82.8% accuracy of the predictions\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape\n",
    "\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro F1 score\n",
    "\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro F1 score\n",
    "\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted F1 score\n",
    "\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average F1 score\n",
    "\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLogistic Regression Accuracy: ' + str(accuracy_score(y_test, y_pred)*100))\n",
    "print('Logistic Regression Classification report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Random Forests - accuracy 81.89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model creation\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRandom Forest Accuracy: ' + str(accuracy_score(y_test, model)*100))\n",
    "print('Random Forest Classification report:\\n', classification_report(y_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: Polynomial Kernel SVM, Accuracy = 82.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=1, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "\n",
    "# y_trainRaveled=y_train.ravel()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "print('Polynomial Regression Accuracy: ' + str(accuracy_score(y_test, y_pred)*100))\n",
    "print('Polynomial Regression Classification report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "results = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "print (\"Confusion Matrix :\")\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5: Using svm.SVC kernel='linear', accuracy = 82.82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an svm Classifier\n",
    "model = svmClassifier2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=0.1, loss='hinge',random_state=42))\n",
    "                          ])\n",
    "\n",
    "#2. Train the model using the training sets - fit the model - with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#3. Predict the response for test dataset - predict using the trained model for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Linear SVM Accuracy: ' + str(accuracy_score(y_test, y_pred)*100))\n",
    "print('Linear SVM Classification report:\\n', classification_report(y_test, y_pred))\n",
    "\n",
    "results = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "print (\"Confusion Matrix :\")\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    'micro':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "'weighted':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the preprocessed data for Casualities\n",
    "# We observe that the casualities data has timestamp in the accident_index. Using this, we want to do time series.\n",
    "\n",
    "df = pd.read_csv(\"../data/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Accident_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['Accident_Index'].astype(str)\n",
    "\n",
    "df['Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['Year'].str[0:4]\n",
    "\n",
    "df['Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Year').count()\n",
    "\n",
    "#We could observe that the number of accidents in 2019 was the lowest of all the data that we have.\n",
    "\n",
    "# 2019 - 144552\n",
    "\n",
    "# 2018 - 151955\n",
    "\n",
    "# 2017 - 162341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x axis values \n",
    "x = [2017,2018,2019] \n",
    "# corresponding y axis values \n",
    "y = [162341,151955,144552] \n",
    "  \n",
    "# plotting the points  \n",
    "plt.plot(x, y) \n",
    "  \n",
    "# naming the x axis \n",
    "plt.xlabel('Year') \n",
    "# naming the y axis \n",
    "plt.ylabel('Number of Accidents That Happened') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Accidents in a Year') \n",
    "  \n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
